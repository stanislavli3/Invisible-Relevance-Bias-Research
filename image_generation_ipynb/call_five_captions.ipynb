{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import joblib\n","import json\n","from tqdm import tqdm\n","from transformers import AutoProcessor, AutoModelForPreTraining\n","import torch"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_name_or_path = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n","processor = AutoProcessor.from_pretrained(model_name_or_path)\n","model = AutoModelForPreTraining.from_pretrained(model_name_or_path)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["file = \"/Users/stanislav/Invisible-Relevance-Bias/flickr/Flickr30k/captions.txt\"\n","new_file = \"./flickr_merge/flickr30k_test_llama_caps.txt\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["output_dir = os.path.dirname(new_file)\n","if not os.path.exists(output_dir):\n","    os.makedirs(output_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["with open(file, 'r') as f, open(new_file, 'w') as f_write:\n","    # List to store the final processed data\n","    data_final = []\n","\n","    # Prompt to consolidate captions\n","    prompt_template = (\n","        'Consolidate the five descriptions, avoid redundancy while including the scene described in each sentence, '\n","        'and make a concise summary:\\n'\n","    )\n","\n","    # Limit the number of API requests to avoid overloading\n","    MAX_REQUESTS = 10\n","\n","    # Read all lines from the file\n","    lines = f.readlines()\n","\n","    # Iterate over each line (with tqdm for progress tracking)\n","    for idx, line in tqdm(enumerate(lines[:MAX_REQUESTS]), total=MAX_REQUESTS):\n","        # Check if the data is already processed\n","        if idx < len(data_final):\n","            continue\n","\n","        # Split the line into parts\n","        parts = line.strip().split(',')\n","\n","        # Ensure that we have the correct format (image_name, label, and captions)\n","        if len(parts) < 3:\n","            print(f\"Skipping line {idx} due to incorrect format\")\n","            continue\n","\n","        # Prepare data structure\n","        new_one_data = {\n","            'image_name': parts[0],  # First part is the image name\n","            'label': parts[1],       # Second part is the label\n","            'caption': []            # We will store the new consolidated caption here\n","        }\n","\n","        # Captions (parts[2:] handles the case of commas in the caption)\n","        captions = parts[2:]\n","\n","        # Format the text for the prompt\n","        text = prompt_template + \"\\n\".join([f\"{i + 1}. {caption}\" for i, caption in enumerate(captions)])\n","\n","        # Tokenize the input text for the model\n","        inputs = processor(text, return_tensors=\"pt\").to(device)\n","\n","        # Generate the caption using LLaMA\n","        outputs = model.generate(**inputs, max_length=100, temperature=0.7)\n","        new_text = processor.decode(outputs[0], skip_special_tokens=True)\n","\n","        # Save the generated caption\n","        new_one_data['caption'] = new_text\n","\n","        # Write the new processed data to the output file\n","        f_write.write(json.dumps(new_one_data) + \"\\n\")\n","\n","        # Append the new processed data to the final list\n","        data_final.append(new_one_data)\n","\n","        # Save intermediate results\n","        joblib.dump(data_final, './flickr_merge/flickr30k_test_llama_caps')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Processing completed and file saved.\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":2}
