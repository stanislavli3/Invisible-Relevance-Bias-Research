{"cells": [{"cell_type": "markdown", "metadata": {}, "source": [" \u4f7f\u7528CLIP Large \u7b5b\u9009\u751f\u6210\u7684\u591a\u4e2a\u56fe\u50cf\u4e2d\u6700\u597d\u7684\u90a3\u4e2a(\u548c\u539f\u56fe\u76f8\u4f3c\u5ea6\u6700\u9ad8\u7684)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import json\n", "import os\n", "from shutil import copyfile\n", "from tqdm import tqdm\n", "import torch\n", "from transformers import CLIPProcessor, CLIPModel\n", "from PIL import Image\n", "os.environ['CUDA_VISIBLE_DEVICES']='1'\n", "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if __name__ == '__main__':\n", "    model = CLIPModel.from_pretrained(\"/data/houdanyang/data/openclip_vit_h\").cuda()\n", "    preprocesor = CLIPProcessor.from_pretrained(\"/data/houdanyang/data/openclip_vit_h\")\n", "    file = \"./flickr_merge/flickr30k_test_chatgpt_caps.txt\"\n", "    f = open(file)\n", "    data_list = f.readlines()\n", "    paths = []\n", "    for i in range(0,30):\n", "        paths.append('./flickr_merge/test_fp16_refiner_{}'.format(i))\n", "    paths.append('./flickr/flickr30k-images')\n", "    save_file = \"./flickr_merge/selected_image_results_30_open_clip_h14.json\"\n", "    path_selected = \"./flickr_merge/test_fp16_refiner_selected_30_open_clip_h14\"\n", "    if not os.path.exists(path_selected):\n", "        os.makedirs(path_selected)\n", "    selected_image_results = {}\n", "    image_names = []\n", "    caps = []\n", "    images = []\n", "    print('loading caption')\n", "    for line in tqdm(data_list):\n", "        one_data = json.loads(line)\n", "        text = one_data['caption']\n", "        caps.append(text)\n", "        image_names.append(one_data['image'].split('/')[-1]) # name of the image for the caption\n", "        for path in paths:\n", "            image_file = os.path.join(path, one_data['image'].split('/')[-1]) #load generated image and real image\n", "            images.append(preprocesor.image_processor(Image.open(image_file).convert(\"RGB\"), return_tensors='pt').data['pixel_values'][0])\n", "    images = torch.stack(images)\n", "    images_features = []\n", "    print('caculating similarity')\n", "    with torch.no_grad():\n", "        for i in tqdm(range(0, len(images), 128)):\n", "            image_batch = images[i:i+128].cuda()\n", "            image_batch = model.get_image_features(image_batch)\n", "            image_batch = image_batch / image_batch.norm(p=2, dim=-1, keepdim=True)\n", "            images_features.append(image_batch)\n", "        images_features = torch.cat(images_features)\n", "        images_features = images_features.reshape(len(data_list),  -1, images_features.shape[-1])\n", "        images_features_real = images_features[:,-1,:].unsqueeze(1) #real images\n", "        images_features_generated = images_features[:,:-1,:] #generated images\n", "        similarities = (images_features_real * images_features_generated).sum(-1)\n", "        _, selected_index = similarities.max(-1)\n", "    for idx,image_name in enumerate(image_names):\n", "        selected_image_results[image_name] = similarities[idx].tolist()\n", "        max_similar_image_path = paths[selected_index[idx]]\n", "        copyfile(os.path.join(max_similar_image_path, image_name), os.path.join(path_selected, image_name))\n", "    f=open(save_file,'w')\n", "    f.write(json.dumps(selected_image_results))"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}